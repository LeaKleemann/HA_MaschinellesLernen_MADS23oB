{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hausarbeit Maschinelles Lernen MADS23oB Heidi Gehring, Lea Kleemann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eidesstattliche Erklärung\n",
    "Hiermit versichere ich _________________________________,\n",
    "_________________, dass ich sämtliche im\n",
    "___________________________________ schriftlich\n",
    "Masterstudiengang\n",
    "angefertigten\n",
    "Prüfungsleistungen, die ich in elektronischer oder anderer Form an der\n",
    "NORDAKADEMIE vorlegen werde, ohne fremde Hilfe und ohne Benutzung anderer als der\n",
    "angegebenen Hilfsmittel anfertigen werde. Die aus fremden Quellen direkt oder indirekt\n",
    "übernommenen Gedanken werden als solche kenntlich gemacht. Die Arbeiten wurden bisher\n",
    "in gleicher oder ähnlicher Form noch keiner anderen Prüfungskommission vorgelegt oder\n",
    "veröffentlicht. Für meine Masterthesis werde ich eine gesonderte Erklärung bei der\n",
    "Abgabe anfertigen und unterzeichnet in die Arbeit einbinden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemstellung\n",
    "### Gewählter Datensatz\n",
    "In dieser Arbeit wird [folgender](https://www.kaggle.com/datasets/thedevastator/book-recommender-system-itembased?select=collaborative_books_df.csv) Datensatz verwendet. Dieser zielt ursprünglich auf die Entwicklung eines Recommender Systems für Buchvorschläge ab. \n",
    "\n",
    "Der Datensatz besteht aus Informationen über Bücher, Bewertungen und Nutzer, die von Kaggle stammen. Der Datensatz enthält Merkmale wie Buch-ID, Titel, Autor, ISBN, Genre, Bewertung, und Nutzer-ID.\n",
    "### Analysefrage\n",
    "Im Rahmen dieser Arbeit soll evaluiert werden, ob die vorhandenen Daten sich eignen, vorherzusagen, welche Bewertung ein Nutzer einem gegebenen Buch geben würde.\n",
    "### Link zum Repository\n",
    "Das für diese Arbeit genutzte GitHub-Repository ist unter [folgender](https://github.com/LeaKleemann/HA_MaschinellesLernen_MADS23oB) Adresse zu finden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lösungsansatz und Projektskizze\n",
    "Ziel dieser Arbeit ist, für einen Nutzer und ein Buch eine Bewertungsvorhersage analog der \"Predicted Rating\" aus collaborative_books_df.csv zu treffen.\n",
    "Es soll kein Recommender System erstellt werden.\n",
    "\n",
    "Im Rahmen dieser Arbeit sollen sowohl Classification- als auch Regressions-Algorithmen für diesen Anwendungsfall untersucht werden. Hier bei dienen die \"Actual Rating\"-Bewertungen als Target-Werte. Da diese sowohl als eine finite Anzahl Kategorien, als auch als numerische Werte aufgefasst werden können, bieten sich beide Ansätze an.\n",
    "\n",
    "Diese Arbeit startet mit der Hypothese, dass die Nutzer anhand ihres Lese- und Bewertungsverhaltens in Lesergruppen geclustert werden können. Diese Cluster sollen anstelle der Nutzer-ID im Berwertungsvorhersagemodell als Feature verwendet werden.\n",
    "\n",
    "Des Weiteren müssen einige Buch-Merkmale in feste Dimensionalitäten gebracht werden. Hierbei lag das Augenmerk auf den Merkmalen \"description\" mit dem Klappentext und \"genre\", welches eine Liste aus Genres, denen ein Buch angehört, beinhaltet. Für den Klappentext werden Topic-Modeling-Ansätze herangezogen und für die Genres wird One-Hot-Encoding genutzt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenexploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all imports for the notebook\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import pathlib2 as pathlib\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import tokenize\n",
    "import re\n",
    "\n",
    "import joblib\n",
    "from optuna import Trial, create_study\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "randomstate=313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base paths\n",
    "cwd=pathlib.Path.cwd() #current working directory\n",
    "datadirpath=cwd.joinpath(\"data\") # data drectory unter current working directory\n",
    "rawdatapath=datadirpath.joinpath(\"raw\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore collaborative_books_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewdf=pd.read_csv(rawdatapath.joinpath(\"collaborative_books_df.csv\"))\n",
    "reviewdf=reviewdf.drop([reviewdf.columns[0]],axis=1) # removed unnamed index column\n",
    "print(reviewdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print basic information\n",
    "print(f\"Number of observations: {len(reviewdf)}\")\n",
    "print(f\"Number of unique book ids: {len(reviewdf[\"book_id\"].unique())}\")\n",
    "print(f\"Number of unique user ids: {len(reviewdf[\"user_id_mapping\"].unique())}\")\n",
    "print(f\"Average actual rating: {reviewdf[\"Actual Rating\"].mean()}\")\n",
    "print(f\"Average predicted rating: {reviewdf[\"Predicted Rating\"].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewdf[\"book_id\"].plot() # not conclusive, try grouped by id, count as aggregate metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_books=len(reviewdf.book_id_mapping.unique())\n",
    "print(f'Number of unique books: {num_books}') #898 --> that means we do not have data for all books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore user ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby user id\n",
    "reviewdf_userids=reviewdf[[\"user_id_mapping\",\"book_id\"]].groupby(by=\"user_id_mapping\").count()\n",
    "reviewdf_userids=reviewdf_userids.reset_index()\n",
    "reviewdf_userids=reviewdf_userids.rename(columns={\"book_id\":\"book_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"(repeat) Number of users: {len(reviewdf_userids)}\")\n",
    "print(f\"Max number of books revied per user: {reviewdf_userids.book_count.max()}\")\n",
    "print(f\"Min number of books revied per user: {reviewdf_userids.book_count.min()}\")\n",
    "print(f\"Number of users with 1 review: {len(reviewdf_userids.query('book_count == 3'))}\")\n",
    "print(f\"... in %: {round(len(reviewdf_userids.query('book_count == 3'))/len(reviewdf_userids),2)*100}%\") # 18% is suboptimal --> prediction of score for single user is probably not going to work\n",
    "print(f\"Number of users with less than 6 reviews: {len(reviewdf_userids.query('book_count < 6'))}\")\n",
    "print(f\"... in %: {round(len(reviewdf_userids.query('book_count < 6'))/len(reviewdf_userids),2)*100}%\") # 90% is suboptimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore book ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupby book_id\n",
    "reviewdf_bookids=reviewdf[[\"book_id\",\"user_id_mapping\"]].groupby(by=\"book_id\").count()\n",
    "reviewdf_bookids=reviewdf_bookids.reset_index()\n",
    "reviewdf_bookids=reviewdf_bookids.rename(columns={\"user_id_mapping\":\"user_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"(repeat) Number of books: {len(reviewdf_bookids)}\")\n",
    "print(f\"Max number of users reviewd per book: {reviewdf_bookids.user_count.max()}\")\n",
    "print(f\"Min number of users reviewd per book: {reviewdf_bookids.user_count.min()}\")\n",
    "print(f\"Number of books with 1 review: {len(reviewdf_bookids.query('user_count == 3'))}\")\n",
    "print(f\"... in %: {round(len(reviewdf_bookids.query('user_count == 3'))/len(reviewdf_bookids),2)*100}%\") # 18% is suboptimal --> prediction of score for single user is probably not going to work\n",
    "print(f\"Number of books with less than 6 reviews: {len(reviewdf_bookids.query('user_count < 6'))}\")\n",
    "print(f\"... in %: {round(len(reviewdf_bookids.query('user_count < 6'))/len(reviewdf_bookids),2)*100}%\") # 90% is suboptimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewvaluecountsdf=reviewdf[\"Actual Rating\"].value_counts()\n",
    "reviewssum=reviewvaluecountsdf.sum()\n",
    "reviewvaluecountsdf[\"percentage\"]=reviewvaluecountsdf.transform(func=lambda x:x/reviewssum*100,axis=0) #50+% at rating of more than 3\n",
    "\n",
    "predvaluecountsdf=reviewdf[\"Predicted Rating\"].value_counts()\n",
    "predvaluecountsdf[\"percentage\"]=predvaluecountsdf.transform(func=lambda x:x/reviewssum*100,axis=0)\n",
    "\n",
    "bins=[1,2,3,4,5,6] #\n",
    "hist, edges = np.histogram(reviewdf[\"Predicted Rating\"],bins=bins,density=False)\n",
    "predbinsvaluecountsdf=pd.DataFrame(hist,index=bins[:-1],columns=[\"PredictedRating_count\"])\n",
    "predbinsvaluecountsdf[\"percentage\"]=predbinsvaluecountsdf.transform(func=lambda x:x/reviewssum*100,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewvaluecountsdf[\"percentage\"].plot(kind='pie',colormap='Accent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predbinsvaluecountsdf.percentage.plot(kind='pie',colormap='Accent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sum of reviews: {reviewssum}\")\n",
    "\n",
    "print(f\"(repeat) Average actual rating: \\n {reviewdf[\"Actual Rating\"].mean()}\")\n",
    "print(f\"Max actual rating: {reviewdf[\"Actual Rating\"].max()}\")\n",
    "print(f\"Min actual rating: {reviewdf[\"Actual Rating\"].min()}\")\n",
    "print(f\"Overview of Rating occurence: {reviewvaluecountsdf}\")\n",
    "print(f\"... in percentage: \\n {reviewvaluecountsdf[\"percentage\"]}\") # comparatively few bad reviews\n",
    "print(\"________________ \\n\")\n",
    "print(f\"(repeat) Average predicted rating: {reviewdf[\"Predicted Rating\"].mean()}\")\n",
    "print(f\"Max predicted rating: {reviewdf[\"Predicted Rating\"].max()}\")\n",
    "print(f\"Min predicted rating: {reviewdf[\"Predicted Rating\"].min()}\")\n",
    "# print(f\"Overview of Rating occurence: {predvaluecountsdf}\") # continuous values\n",
    "# print(f\"... in percentage: \\n {predvaluecountsdf[\"percentage\"]}\") # continuous values, try binning\n",
    "print(f\"Overview of Rating occurence: \\n {predbinsvaluecountsdf[\"PredictedRating_count\"]}\")\n",
    "print(f\"... in percentage: \\n {predbinsvaluecountsdf[\"percentage\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploring collaborative_book_metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksdf=pd.read_csv(rawdatapath.joinpath('collaborative_book_metadata.csv'))\n",
    "booksdf=booksdf.drop([booksdf.columns[0]],axis=1) # removed unnamed index column\n",
    "print(booksdf.columns)\n",
    "booksgroupbyauthorsdf=booksdf[[\"name\",\"book_id_mapping\"]].groupby(by=\"name\").count()\n",
    "booksgroupbyauthorsdf=booksgroupbyauthorsdf.reset_index()\n",
    "booksgroupbyauthorsdf=booksgroupbyauthorsdf.rename(columns={\"book_id_mapping\":\"book_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booksgroupbyauthorsdf.sort_values(by=\"book_count\",ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of books: {len(booksdf)}\")\n",
    "print(\"___________________ \\n\")\n",
    "print(f\"Max number of pages: {booksdf.num_pages.max()}\")\n",
    "print(f\"Avg number of pages: {booksdf.num_pages.mean()}\")\n",
    "print(f\"Min number of pages: {booksdf.num_pages.min()}\")\n",
    "print(\"___________________ \\n\")\n",
    "print(f\"Max number of ratings: {booksdf.ratings_count.max()}\")\n",
    "print(f\"Avg number of ratings: {booksdf.ratings_count.mean()}\")\n",
    "print(f\"Min number of ratings: {booksdf.ratings_count.min()}\")\n",
    "print(\"___________________ \\n\")\n",
    "print(f\"Number of unique authors: {len(booksdf.name.unique())}\")\n",
    "print(f\"Authors with most books: {booksgroupbyauthorsdf.sort_values(by=\"book_count\",ascending=False).head()}\") #max =3, not al lot of books per author, maybe not predictive enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explore genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and process genres\n",
    "booksdf['genre']=booksdf.genre.apply(lambda x: x.replace(\"'\",\"\")) # remove '\n",
    "booksdf['genre']=booksdf.genre.apply(lambda x: x.replace(\"-\",\"\")) # revome - \n",
    "booksdf['genre']=booksdf.genre.apply(lambda x: x.replace(\" \",\"\")) # remove spaces\n",
    "booksdf['genre']=booksdf.genre.apply(lambda x: x[1:-1].split(',')) # split into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of genres\n",
    "booksdf[\"num_genres\"]=booksdf.genre.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max number of genres: {booksdf.num_genres.max()}\")\n",
    "print(f\"Avg number of genres: {booksdf.num_genres.mean()}\")\n",
    "print(f\"Min number of genres: {booksdf.num_genres.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenedgenres=[g for sub in booksdf.genre.to_list() for g in sub]\n",
    "uniquegenres=list(set(flattenedgenres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uniquegenres)\n",
    "print(f\"Number of unique genres: {len(uniquegenres)}\") # 16 genres: maybe too many --> sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode genres\n",
    "booksdf=booksdf.join(booksdf.genre.str.join('|').str.get_dummies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genressum=booksdf[uniquegenres].sum()\n",
    "genressum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max amount of books in genre: {genressum.max()}\") # fiction, with 96 books total and 16 nonfiction? are there fiction/nonfiction books?\n",
    "print(f\"Avg amount of books in genre: {genressum.mean()}\")\n",
    "print(f\"Min amount of books in genre: {genressum.min()}\") # poetry only has 5 books, could be underrepresented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contradictorygenresdf=booksdf[(booksdf['nonfiction']==1) & (booksdf['fiction']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of books both fiction and nonfiction: {len(contradictorygenresdf)}\") # 12, consider removing fiction-genre?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join books_df (reviews) with book_metadata (books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedouterdf=pd.merge(reviewdf,booksdf,left_on='book_id_mapping',right_on='book_id_mapping',how='outer')\n",
    "joinedinnerdf=pd.merge(reviewdf,booksdf,left_on='book_id_mapping',right_on='book_id_mapping',how='inner')\n",
    "\n",
    "tempdupesdf=pd.concat([joinedouterdf[['book_id_mapping','user_id_mapping']],joinedinnerdf[['book_id_mapping','user_id_mapping']]],).drop_duplicates(keep=False) # get non-matches, len 176 473 is correct length\n",
    "differencedf=joinedouterdf.drop(index=tempdupesdf.index)\n",
    "\n",
    "print(len(joinedouterdf))\n",
    "print(len(joinedinnerdf))\n",
    "len(joinedouterdf)-len(joinedinnerdf) # check number of non matches\n",
    "\n",
    "print(len(differencedf))\n",
    "print(len(tempdupesdf))\n",
    "len(tempdupesdf)+len(differencedf) # checked with above so lengths match\n",
    "\n",
    "tempcheckdf=joinedinnerdf.duplicated(tempdupesdf,False)\n",
    "tempcheckdf.value_counts() # only False --> no dupes, code worked correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save new dataframes to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processeddatapath=datadirpath.joinpath('processed')\n",
    "try: \n",
    "    processeddatapath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "outerjoinpath=processeddatapath.joinpath('outerJoinData.csv')\n",
    "innerjoinpath=processeddatapath.joinpath('innerJoinData.csv')\n",
    "differencejoinpath=processeddatapath.joinpath('differenceJoinData.csv')\n",
    "dupesjoinpath=processeddatapath.joinpath('duplicatesJoinData.csv')\n",
    "booksdfpath=rawdatapath.joinpath('collaborative_book_metadata_with_genredummies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedouterdf.to_csv(outerjoinpath,sep=';',index=False)\n",
    "joinedinnerdf.to_csv(innerjoinpath,sep=';',index=False)\n",
    "differencedf.to_csv(differencejoinpath,sep=';',index=False)\n",
    "tempdupesdf.to_csv(dupesjoinpath,sep=';',index=False)\n",
    "booksdf.to_csv(booksdfpath,sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Clustern\n",
    "### Nutzerdaten\n",
    "### Topic Modeling der Buchklappentexte\n",
    "### Erkenntnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataworeviewsfile=datadirpath.joinpath('raw/collaborative_book_metadata_with_genredummies.csv')\n",
    "models_path=cwd.joinpath('models')\n",
    "\n",
    "fulldatadf=pd.read_csv(dataworeviewsfile,sep=';')\n",
    "# remove previously created columns by pd.get_dummies()\n",
    "fulldatadf=fulldatadf.drop(['biography', 'children', 'comics', 'crime', 'fantasy', 'fiction',\n",
    "       'graphic', 'historicalfiction', 'history', 'mystery', 'nonfiction',\n",
    "       'paranormal', 'poetry', 'romance', 'thriller', 'youngadult'],axis=1)\n",
    "\n",
    "# preporcess genre string to list of genres\n",
    "fulldatadf['genre']=fulldatadf.genre.apply(lambda x: x.replace(\"'\",\"\")) # remove '\n",
    "fulldatadf['genre']=fulldatadf.genre.apply(lambda x: x.replace(\"-\",\"\")) # revome - \n",
    "fulldatadf['genre']=fulldatadf.genre.apply(lambda x: x.replace(\" \",\"\")) # remove spaces\n",
    "fulldatadf['genre']=fulldatadf.genre.apply(lambda x: x[1:-1].split(',')) # split into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or Load MultiLabelBinarizer for One Hot Encoding of genres\n",
    "retrain_mlb=True\n",
    "mlbmodelpath=models_path.joinpath('mlb_model')\n",
    "if retrain_mlb:\n",
    "    xtrain,xtest=train_test_split(fulldatadf['genre'],test_size=0.1,random_state=randomstate,shuffle=True)\n",
    "    mlb=MultiLabelBinarizer()\n",
    "    mlbmodel=mlb.fit(xtrain)\n",
    "    joblib.dump(mlbmodel,mlbmodelpath.joinpath('mlbmodel.pkl').as_posix())\n",
    "else:\n",
    "    mlbmodel=joblib.load(mlbmodelpath.joinpath('mlbmodel.pkl').as_posix())\n",
    "\n",
    "newcols=mlbmodel.classes_\n",
    "\n",
    "pred=mlbmodel.transform(fulldatadf.genre)\n",
    "preddf=pd.DataFrame(pred,columns=newcols)\n",
    "fulldatadf=fulldatadf.join(preddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzerdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heidi Gehring: loading user ratings and inspecting the data\n",
    "user_ratings_df=pd.read_csv(rawdatapath.joinpath(\"collaborative_books_df.csv\"))\n",
    "user_ratings_df=user_ratings_df.drop([user_ratings_df.columns[0]],axis=1) # removed unnamed index column\n",
    "print(user_ratings_df.columns)\n",
    "print(user_ratings_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heidi Gehring: Count unique entries in 'user_id_mapping' to understand how many unique users there are\n",
    "unique_count = user_ratings_df['user_id_mapping'].nunique()\n",
    "\n",
    "print(f\"Number of unique entries: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heidi Gehring: loading and inspecting the metadata about books\n",
    "book_metadata_df=pd.read_csv(rawdatapath.joinpath(\"collaborative_book_metadata.csv\"))\n",
    "book_metadata_df=book_metadata_df.drop([book_metadata_df.columns[0]],axis=1) # removed unnamed index column\n",
    "print(book_metadata_df.columns)\n",
    "print(book_metadata_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation:\n",
    "Merge the 2 datasets and only keep the user ratings, where we have book metadata.\n",
    "\n",
    "Going forward the merged dataset will be explored to understand whether it is suitbable to predict how users would rate books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heidi Gehring: prepare data about books to be merged\n",
    "\n",
    "# Function to clean up genre names\n",
    "def clean_genre(genre):\n",
    "    if pd.isna(genre):\n",
    "        return ''\n",
    "    genre = genre.strip()\n",
    "    genre = re.sub(r\"[^\\w\\s,]\", \"\", genre)  # Remove any non-alphanumeric characters except commas and spaces\n",
    "    genre = genre.replace(\"'\", \"\")          # Remove single quotes\n",
    "    genre = genre.replace(\"[\", \"\")          # Remove square brackets\n",
    "    genre = genre.replace(\"]\", \"\")\n",
    "    genre = genre.replace(\",\", \", \")        # Normalize spaces around commas\n",
    "    return genre\n",
    "\n",
    "# Apply the cleaning function to the genre column\n",
    "book_metadata_df['cleaned_genre'] = book_metadata_df['genre'].apply(clean_genre)\n",
    "\n",
    "# Display cleaned genres to verify\n",
    "print(\"Cleaned Genres:\")\n",
    "print(book_metadata_df['cleaned_genre'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one hot encoding to be able to use the genre information for clustering and prediction, going forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding the genre information\n",
    "genre_dummies = book_metadata_df['cleaned_genre'].str.get_dummies(sep=', ')\n",
    "\n",
    "# Combine the encoded genre information with the book metadata\n",
    "book_metadata_df = pd.concat([book_metadata_df, genre_dummies], axis=1)\n",
    "\n",
    "# Drop the original genre and cleaned_genre columns as it is no longer needed\n",
    "book_metadata_df = book_metadata_df.drop(columns=['genre', 'cleaned_genre'])\n",
    "\n",
    "# Display the book metadata and inspect the new columns\n",
    "print(\"Processed Book Metadata DataFrame Head:\")\n",
    "print(book_metadata_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heidi Gehring: regularly use print to inspect the current dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_metadata_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heidi Gehring: Merge the user ratings DataFrame with the book metadata DataFrame\n",
    "# Use 'book_id_mapping' as the common key, and only keep rows with metadata\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = pd.merge(user_ratings_df, book_metadata_df, on='book_id_mapping', how='inner')\n",
    "\n",
    "# The 'inner' join ensures that only rows with matching 'book_id_mapping' in both DataFrames are kept.\n",
    "# This automatically removes any rows from user_ratings_df that do not have corresponding metadata.\n",
    "\n",
    "# Display the DataFrame to verify\n",
    "print(\"Merged and Filtered DataFrame Head:\")\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique entries in 'column_name'\n",
    "unique_count1 = merged_df['user_id_mapping'].nunique()\n",
    "\n",
    "print(f\"Number of unique entries: {unique_count1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heidi Gehring: Objective is to create 1 vector per user. This will then be used as input to create user clusters\n",
    "\n",
    "Creating user clusters:\n",
    "Intially normalized data was used, but it did not yield good results for the clusters. Then the \"non-normalized\" data was used. It also did not yield good results for the clusters. So it was decided to move into the prediction without clustering the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heidi Gehring: reduce the data to 1 vector per user for further processing\n",
    "\n",
    "# Step 1: Create the user-item matrix combined with book metadata\n",
    "user_item_with_metadata = merged_df.copy()\n",
    "\n",
    "# Identify numeric columns (metadata columns plus genres)\n",
    "numeric_columns = ['num_pages', 'ratings_count'] + [col for col in genre_dummies.columns if pd.api.types.is_numeric_dtype(genre_dummies[col])]\n",
    "\n",
    "# Step 2: Multiply each user's rating by the corresponding numeric metadata column\n",
    "# Filter out non-numeric columns from `user_item_with_metadata`\n",
    "numeric_metadata_columns = [col for col in numeric_columns if col in user_item_with_metadata.columns]\n",
    "\n",
    "# Ensure 'Actual Rating' is numeric\n",
    "user_item_with_metadata['Actual Rating'] = pd.to_numeric(user_item_with_metadata['Actual Rating'], errors='coerce')\n",
    "\n",
    "# Apply multiplication only on numeric columns\n",
    "for col in numeric_metadata_columns:\n",
    "    if col in user_item_with_metadata.columns:\n",
    "        user_item_with_metadata[col] = user_item_with_metadata['Actual Rating'] * user_item_with_metadata[col]\n",
    "\n",
    "# Drop non-numeric columns before aggregation\n",
    "user_item_with_metadata_numeric = user_item_with_metadata[numeric_metadata_columns + ['user_id_mapping']]\n",
    "\n",
    "# Step 3: Aggregate the vectors for each user\n",
    "user_vectors = user_item_with_metadata_numeric.groupby('user_id_mapping').mean()\n",
    "\n",
    "# Step 4: Normalize the vectors, was initially executed, and in the second iteration not. \n",
    "#user_vectors_normalized = pd.DataFrame(\n",
    " #   normalize(user_vectors, norm='l2'),\n",
    "  #  index=user_vectors.index,\n",
    "   # columns=user_vectors.columns\n",
    "#)\n",
    "\n",
    "# Step 5: Review the final user vectors\n",
    "#print(\"Aggregated and Normalized User Vectors Head:\") # not needed here as it was decided to try without normalization\n",
    "#print(user_vectors_normalized.head())\n",
    "print(user_vectors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect the shape of the newly created user vectors\n",
    "print(user_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the users\n",
    "The hypothesis was, that by creating clusters, it might be possible to then use the information for each cluster to predict user ratings. \n",
    "The result was, that no good clustering was possible. \n",
    "\n",
    "Some iterations in the code were tried out with the cluster range to come to a conclusion how many cllusters might be optimal.\n",
    "\n",
    "According to the elbow method and the Silhouette method, a cluster number of 25 might already give an acceptable result, but expereimenting shows that neither with 25 nor with 75 clusters, satisfying results can be achieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heidi Gehring: Define the range for the number of clusters\n",
    "cluster_range = range(10, 71)  # Adjust this range if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimenting with 20, 30 and 50 cluster range, 20 was very inconclusive, 30 was better \n",
    "Looking at a range up to 50 or 75 leads to the conclusion that 25 clusters could be a good number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow method to find out the right number of clusters.\n",
    "\n",
    "# Compute K-Means for each number of clusters\n",
    "inertia = []\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(user_vectors)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silhouette score as alternative method to look at potential number of clusters\n",
    "\n",
    "silhouette_scores = []\n",
    "for n_clusters in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(user_vectors)\n",
    "    silhouette_avg = silhouette_score(user_vectors, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot the Silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Running several versions of elbow and silhouette method with different cluster ranges (up to 70 clusters)  leads to the initial decision to go forward with 25 clusters, as it seems to be a reasonable compromise of acceptable scores and a manageable number of clusters.\n",
    "\n",
    " Plotting the 25 clusters shows, that the clusters are not well separated. \n",
    "\n",
    " Trying out different numbers of clusters leads to the conclusion, that with up to 70 clusters, no good separation can be achieved. \n",
    "\n",
    " Both 2D and 3D Visualization were applied to inspect the outcomes. No good outcomes were achieved in any visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setting the number of clusters to be calculated, can be changed to expect different outcomes\n",
    "optimal_n_clusters = 35\n",
    "\n",
    "# Fit K-Means model with the chosen number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=0)\n",
    "user_vectors['cluster'] = kmeans.fit_predict(user_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster profiles\n",
    "cluster_profiles = user_vectors.groupby('cluster').mean()\n",
    "print(cluster_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially the viridis color palette was used, but this is very monotonic.\n",
    "A change to Paired helped the visualization in general, but not the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(user_vectors.drop('cluster', axis=1))\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_pca = pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\n",
    "df_pca['cluster'] = user_vectors['cluster']\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='cluster', palette='Paired', legend='full')\n",
    "plt.title('PCA of User Clusters')\n",
    "# Adjust the legend to fit into the visual \n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0, ncol=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating 3d visualization, to see if this leads to a better visualization \n",
    "# # prepare data for it\n",
    "\n",
    "# Perform PCA to reduce to 3D\n",
    "pca = PCA(n_components=3)\n",
    "reduced_data = pca.fit_transform(user_vectors.drop('cluster', axis=1))\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_pca = pd.DataFrame(reduced_data, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca['cluster'] = user_vectors['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3D scatter plot, starting with Viridis again\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(df_pca['PC1'], df_pca['PC2'], df_pca['PC3'], \n",
    "                     c=df_pca['cluster'], cmap='hsv_r', alpha=0.6)\n",
    "\n",
    "# Add color bar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Cluster')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('PCA of User Clusters in 3D')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the approach using K-Means did not yield statisfying results, another approach was tried, using the DB Scan method.\n",
    "In general, the same steps as for the first clusterin appraoch were followed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the same steps as for K-Means Clustering, steps are repeated as normalization in step 4 is used here\n",
    "# Step 1: Create the user-item matrix combined with book metadata\n",
    "user_item_with_metadata = merged_df.copy()\n",
    "\n",
    "# Identify numeric columns (metadata columns plus genre dummies)\n",
    "numeric_columns = ['num_pages', 'ratings_count'] + [col for col in genre_dummies.columns if pd.api.types.is_numeric_dtype(genre_dummies[col])]\n",
    "\n",
    "# Step 2: Multiply each user's rating by the corresponding numeric metadata column\n",
    "# Filter out non-numeric columns from `user_item_with_metadata`\n",
    "numeric_metadata_columns = [col for col in numeric_columns if col in user_item_with_metadata.columns]\n",
    "\n",
    "# Ensure 'Actual Rating' is numeric\n",
    "user_item_with_metadata['Actual Rating'] = pd.to_numeric(user_item_with_metadata['Actual Rating'], errors='coerce')\n",
    "\n",
    "# Apply multiplication only on numeric columns\n",
    "for col in numeric_metadata_columns:\n",
    "    if col in user_item_with_metadata.columns:\n",
    "        user_item_with_metadata[col] = user_item_with_metadata['Actual Rating'] * user_item_with_metadata[col]\n",
    "\n",
    "# Drop non-numeric columns before aggregation\n",
    "user_item_with_metadata_numeric = user_item_with_metadata[numeric_metadata_columns + ['user_id_mapping']]\n",
    "\n",
    "# Step 3: Aggregate the vectors for each user\n",
    "user_vectors = user_item_with_metadata_numeric.groupby('user_id_mapping').mean()\n",
    "\n",
    "# Step 4: Normalization\n",
    "user_vectors_normalized = pd.DataFrame(\n",
    "   normalize(user_vectors, norm='l2'),\n",
    "   index=user_vectors.index,\n",
    "   columns=user_vectors.columns\n",
    ")\n",
    "\n",
    "# Step 5: Review the final user vectors\n",
    "print(\"Aggregated and Normalized User Vectors Head:\")\n",
    "print(user_vectors.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize the data (important for DBSCAN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(user_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DBSCAN with chosen parameters\n",
    "dbscan = DBSCAN(eps=1, min_samples=5)\n",
    "\n",
    "# Fit the model to the scaled data\n",
    "dbscan.fit(X_scaled)\n",
    "\n",
    "#  labels assigned to each data point\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters (excluding noise)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# Number of noise points\n",
    "n_noise = list(labels).count(-1)\n",
    "\n",
    "print(f'Estimated number of clusters: {n_clusters}')\n",
    "print(f'Estimated number of noise points: {n_noise}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the parameters eps =1 und min samples = 5 a silhousett score of 0,379 was achieved, which is not satisfying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate silhouette score (ignoring noise)\n",
    "silhouette = silhouette_score(X_scaled, labels) if len(set(labels)) > 1 else -1\n",
    "print(f'Silhouette Score: {silhouette}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the random search cv leads to the result that the parameters eps=1 and min-samples = 2 are best. \n",
    "This results in >400 clusters which is also not desired. Therefore the DB Scan clusterin method will not be explored further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random search cv \n",
    "dbscan = DBSCAN()\n",
    "\n",
    "param_dist = {\n",
    "    'eps': np.linspace(1, 5, 5),  # setting the epsilon parameter\n",
    "    'min_samples': range(2, 20)        # setting min samples to form a cluster\n",
    "}\n",
    "\n",
    "\n",
    "def ari_scorer(estimator, X, y_true):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    if len(set(cluster_labels)) > 1:  # Ensure there's more than one cluster\n",
    "        return adjusted_rand_score(y_true, cluster_labels)\n",
    "    else:\n",
    "        return -1  # Assign a low score if clustering is trivial\n",
    "\n",
    "# Example usage in RandomizedSearchCV\n",
    "ari = make_scorer(ari_scorer, needs_proba=False)\n",
    "\n",
    "\n",
    "\n",
    "# RandomizedSearchCV setup\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dbscan,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=100,                   # Number of parameter settings to sample\n",
    "    scoring=ari,           # Use the custom silhouette scoring function\n",
    "    cv=3,                         # Number of folds (not used, but required by RandomizedSearchCV)\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "random_search.fit(X_scaled)\n",
    "\n",
    "# Best hyperparameters and corresponding score\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best Silhouette Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the result verifies that this is really not helpfull in moving forward.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit DB-Scan model \n",
    "dbscan = DBSCAN(eps=0.1, min_samples=2)\n",
    "\n",
    "user_vectors['cluster'] = dbscan.fit_predict(user_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster profiles\n",
    "cluster_profiles = user_vectors.groupby('cluster').mean()\n",
    "print(cluster_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initially it was viridis color palette, but that was very monotonic, changed it to Paired, still not very helpful\n",
    "\n",
    "# Reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(user_vectors.drop('cluster', axis=1))\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_pca = pd.DataFrame(reduced_data, columns=['PC1', 'PC2'])\n",
    "df_pca['cluster'] = user_vectors['cluster']\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', hue='cluster', palette='Paired', legend='full')\n",
    "plt.title('PCA of User Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d visualization\n",
    "\n",
    "# Perform PCA to reduce to 3D\n",
    "pca = PCA(n_components=3)\n",
    "reduced_data = pca.fit_transform(user_vectors_normalized.drop('cluster', axis=1))\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "df_pca = pd.DataFrame(reduced_data, columns=['PC1', 'PC2', 'PC3'])\n",
    "df_pca['cluster'] = user_vectors_normalized['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3D scatter plot, starting with Viridis again\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot\n",
    "scatter = ax.scatter(df_pca['PC1'], df_pca['PC2'], df_pca['PC3'], \n",
    "                     c=df_pca['cluster'], cmap='hsv_r', alpha=0.6)\n",
    "\n",
    "# Add color bar\n",
    "cbar = plt.colorbar(scatter)\n",
    "cbar.set_label('Cluster')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('PCA of User Clusters in 3D')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erkenntnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling of book blurb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    cleanedtext=re.sub('[^A-Za-z0-9]+', ' ', x)\n",
    "    return cleanedtext\n",
    "\n",
    "def tokenize_column(column): # this function tests gesim tokenize function used in vectorizer, not used\n",
    "    l=[]\n",
    "    for x in range(len(column)):\n",
    "        l.append(list(tokenize(column.iloc[x])))\n",
    "    return l\n",
    "\n",
    "fulldatadf['description']=fulldatadf.description.transform(remove_punctuation)\n",
    "# fulldatadf['description_list']=tokenize_column(fulldatadf.description)\n",
    "# fulldatadf['description_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain,xtest=train_test_split(fulldatadf['description'],test_size=0.1,random_state=randomstate,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group books via blurb content\n",
    "- vectorize, topics via topic modeling\n",
    "-  tfidf -> lda\n",
    "    - evaluate via topic coherence\n",
    "- https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/topic-modeling/Evaluate%20Topic%20Models.ipynb\n",
    "- https://medium.com/@walter_sperat/using-optuna-with-sklearn-the-right-way-part-1-6b4ad0ab2451\n",
    "- https://learn-scikit.oneoffcoder.com/optuna.html\n",
    "- https://learn-scikit.oneoffcoder.com/gensim.html\n",
    "- https://stackoverflow.com/questions/60613532/how-do-i-calculate-the-coherence-score-of-an-sklearn-lda-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topiccoherencescorer(pipe):\n",
    "    n_top_words = 10 #higher value for higher coherence, i.e. more word to make connections for coherence\n",
    "    topics=pipe.named_steps['lda'].components_\n",
    "    # texts=[[word for word in doc.split()] for doc in X]\n",
    "    # texts=X\n",
    "    texts=list(pipe.named_steps['vectorizer'].vocabulary_.keys())\n",
    "    dictionary=corpora.Dictionary(texts)\n",
    "    corpus=[dictionary.doc2bow(text) for text in texts]\n",
    "    feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
    "    top_words = []\n",
    "    for topic in topics:\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    cm = CoherenceModel(topics=top_words, texts=texts, dictionary=dictionary, coherence='c_v') #other coherence metrics possible, sticking with c_v\n",
    "    return cm.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optima study for tfidf+lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_studydescription_tfidflda=True\n",
    "studyname='description_tfidflda_study'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "if retrain_studydescription_tfidflda:\n",
    "    xtrain,xtest=train_test_split(fulldatadf['description'],test_size=0.1,random_state=randomstate,shuffle=True)\n",
    "    xtrain,xtest=xtrain.reset_index(drop=True),xtest.reset_index(drop=True) #kf.split() needs reset index; if drop=False, index turned into new column\n",
    "    \n",
    "\n",
    "    def inst_tfidf(trial:Trial)->TfidfVectorizer:\n",
    "        params={\n",
    "            'norm':trial.suggest_categorical('norm',['l1','l2', None]),\n",
    "            'smooth_idf':trial.suggest_categorical('smooth_idf',[True,False]),\n",
    "            'sublinear_tf':trial.suggest_categorical('sublinear_tf',[True,False]),\n",
    "            'stop_words':trial.suggest_categorical('stop_words',['english',list(STOPWORDS)]),\n",
    "            'tokenizer':tokenize,\n",
    "            # 'max_df':trial.suggest_float('max_df',0,1),\n",
    "            # 'min_df':trial.suggest_float('min_df',0,1),\n",
    "            # 'max_features':trial.suggest_categorical('max_features',[None,100,50,25,10]) #can't use int, because of None, PS: no max features, filters out too many words\n",
    "            }\n",
    "        return TfidfVectorizer(**params)\n",
    "    def inst_lda(trial:Trial)->LatentDirichletAllocation:\n",
    "        params={\n",
    "            'n_components':trial.suggest_int('n_components',3,20),\n",
    "            'learning_method':trial.suggest_categorical('learning_method',['batch','online']),\n",
    "            'learning_decay':trial.suggest_float('learning_decay',0.5,0.9),\n",
    "            'learning_offset':trial.suggest_float('learning_offset',2,20),\n",
    "            'max_iter':trial.suggest_int('max_iter',2,20),\n",
    "            'batch_size':trial.suggest_int('batch_size',5,20),\n",
    "            'max_doc_update_iter':trial.suggest_int('max_doc_update_iter',0,10),\n",
    "            'n_jobs':-1,\n",
    "            'random_state':randomstate\n",
    "        }\n",
    "        return LatentDirichletAllocation(**params)\n",
    "    def inst_pipeTFLDA(trial:Trial)->Pipeline:\n",
    "        pipeline=Pipeline([\n",
    "            ('tfidf',inst_tfidf(trial)),\n",
    "            ('lda',inst_lda(trial))\n",
    "        ])\n",
    "        return pipeline\n",
    "\n",
    "    def objective(trial:Trial,xtrain:pd.DataFrame)->float:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=randomstate)\n",
    "        model=inst_pipeTFLDA(trial)\n",
    "        scores=[]\n",
    "        for x in kf.split(xtrain):\n",
    "            subxtrain=xtrain.loc[x[0]]\n",
    "\n",
    "            pipe=model.fit(subxtrain)\n",
    "            score=topiccoherencescorer(pipe) #no cv, as scorer not compatible\n",
    "            scores.append(score)\n",
    "        \n",
    "        # scores = cross_val_score(model, x, cv=kf) #using default scorer --> approx log-likelohood\n",
    "        # return np.min([np.mean(scores), np.median(scores)])\n",
    "        return np.mean(scores)\n",
    "    storage=thismodelpath.joinpath(\"description_tfidflda_study.db\")\n",
    "    if storage.exists():\n",
    "        storage.unlink()\n",
    "    else:\n",
    "        print(\"No sqlite db found\")\n",
    "\n",
    "    study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "    study.optimize(lambda trial: objective(trial,xtrain),n_trials=100,n_jobs=-1,show_progress_bar=True)\n",
    "    joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "    model = inst_pipeTFLDA(study.best_trial)\n",
    "    fitpipe=model.fit(fulldatadf.description)\n",
    "    joblib.dump(fitpipe,thismodelpath.joinpath(f'fittedpipelinedefault.pkl').as_posix())\n",
    "\n",
    "else:\n",
    "    fitpipe=joblib.load(thismodelpath.joinpath(f'fittedpipelinedefault.pkl').as_posix())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10 \n",
    "topics=fitpipe.named_steps['lda'].components_\n",
    "texts=list(fitpipe.named_steps['vectorizer'].vocabulary_.keys())\n",
    "dictionary=corpora.Dictionary(texts)\n",
    "corpus=[dictionary.doc2bow(text) for text in texts]\n",
    "feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
    "top_words = []\n",
    "for topic in topics:\n",
    "    top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "\n",
    "top_words # topics look the same, switch to count vectorizer instead of tfidf?, more preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optima study for cv+lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_studydescription_countveclda=True\n",
    "\n",
    "studyname='description_countlda_study'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "if retrain_studydescription_countveclda:\n",
    "    xtrain,xtest=train_test_split(fulldatadf['description'],test_size=0.1,random_state=randomstate,shuffle=True)\n",
    "    xtrain,xtest=xtrain.reset_index(drop=True),xtest.reset_index(drop=True) #kf.split() needs reset index; if drop=False, index turned into new column\n",
    "\n",
    "    def inst_countvec(trial:Trial)->CountVectorizer:\n",
    "        params={\n",
    "            'stop_words':trial.suggest_categorical('stop_words',['english',list(STOPWORDS)]),\n",
    "            'tokenizer':tokenize,\n",
    "            # 'max_df':trial.suggest_float('max_df',0,1),\n",
    "            # 'min_df':trial.suggest_float('min_df',0,1),\n",
    "            # 'max_features':trial.suggest_categorical('max_features',[None,100,50,25,10]) #can't use int, because of None; do not set! will filter too many words\n",
    "            }\n",
    "        return CountVectorizer(**params)\n",
    "    def inst_lda(trial:Trial)->LatentDirichletAllocation:\n",
    "        params={\n",
    "            'n_components':trial.suggest_int('n_components',3,20),\n",
    "            'learning_method':trial.suggest_categorical('learning_method',['batch','online']),\n",
    "            'learning_decay':trial.suggest_float('learning_decay',0.5,0.9),\n",
    "            'learning_offset':trial.suggest_float('learning_offset',2,20),\n",
    "            'max_iter':trial.suggest_int('max_iter',2,20),\n",
    "            'batch_size':trial.suggest_int('batch_size',5,20),\n",
    "            'max_doc_update_iter':trial.suggest_int('max_doc_update_iter',0,10),\n",
    "            'n_jobs':-1,\n",
    "            'random_state':randomstate\n",
    "        }\n",
    "        return LatentDirichletAllocation(**params)\n",
    "    def inst_pipeTFLDA(trial:Trial)->Pipeline:\n",
    "        pipeline=Pipeline([\n",
    "            ('countvectorizer',inst_countvec(trial)),\n",
    "            ('lda',inst_lda(trial))\n",
    "        ])\n",
    "        return pipeline\n",
    "\n",
    "    def objective(trial:Trial,x:pd.DataFrame)->float:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=randomstate)\n",
    "        model=inst_pipeTFLDA(trial)\n",
    "        scores=[]\n",
    "        for x in kf.split(xtrain):\n",
    "            subxtrain=xtrain.loc[x[0]]\n",
    "\n",
    "            pipe=model.fit(subxtrain)\n",
    "            score=topiccoherencescorer(pipe) #no cv, as scorer not compatible\n",
    "            scores.append(score)\n",
    "            \n",
    "            # scores = cross_val_score(model, x, cv=kf) #using default scorer --> approx log-likelohood\n",
    "        # return np.min([np.mean(scores), np.median(scores)])\n",
    "        return np.mean(scores)\n",
    "\n",
    "    storage=thismodelpath.joinpath(\"description_countveclda_studydefault.db\")\n",
    "    if storage.exists():\n",
    "        storage.unlink()\n",
    "    else:\n",
    "        print(\"No sqlite db found\")\n",
    "\n",
    "    study2=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False) #TPESampler used as default, no pruning\n",
    "    study2.optimize(lambda trial: objective(trial,xtrain),n_trials=100,n_jobs=-1,show_progress_bar=True)\n",
    "    joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "    model = inst_pipeTFLDA(study.best_trial)\n",
    "    fitpipe=model.fit(fulldatadf.description)\n",
    "    joblib.dump(fitpipe,thismodelpath.joinpath(f'fittedpipelinedefault2.pkl').as_posix())\n",
    "else:\n",
    "    fitpipe=joblib.load(thismodelpath.joinpath(f'fittedpipelinedefault2.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 10 \n",
    "topics=fitpipe.named_steps['lda'].components_\n",
    "texts=list(fitpipe.named_steps['vectorizer'].vocabulary_.keys())\n",
    "dictionary=corpora.Dictionary(texts)\n",
    "corpus=[dictionary.doc2bow(text) for text in texts]\n",
    "feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
    "top_words = []\n",
    "for topic in topics:\n",
    "    top_words.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lda with gridsearchcv and nmf as alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizing all hyperparameters makes script run endlessly, does not converge\n",
    "# no max features for vectorizers\n",
    "# nmf has no scorer, no further tries, as to focus on lda\n",
    "retrain_tfidflda=True\n",
    "retrain_cvlda=True\n",
    "\n",
    "cv=CountVectorizer(tokenizer=tokenize)\n",
    "tfidf=TfidfVectorizer(tokenizer=tokenize)\n",
    "lda=LatentDirichletAllocation(n_jobs=-1,random_state=randomstate)\n",
    "nmf=NMF(random_state=randomstate)\n",
    "paramgrida={'vectorizer__stop_words':['english',list(STOPWORDS)],\n",
    "            # 'vectorizer__max_features':[None,100,50],\n",
    "            'lda__n_components':[3,5],\n",
    "            # 'lda__learning_method':['batch','online'],\n",
    "            # 'lda__learning_decay':[0.5,0.9,0.7],\n",
    "            # 'lda__learning_offset':[2,10],\n",
    "            # 'lda__max_iter':[2,10],\n",
    "            # 'lda__batch_size':[10,50],\n",
    "            # 'lda__max_doc_update_iter':[1,5]\n",
    "            }\n",
    "pipelinea=Pipeline([('vectorizer', cv),('lda',lda)])\n",
    "paramgridb={'vectorizer__norm':['l1','l2', None],\n",
    "            'vectorizer__smooth_idf':[True,False],\n",
    "            'vectorizer__sublinear_tf':[True,False],\n",
    "            'vectorizer__stop_words':['english',list(STOPWORDS)],\n",
    "            # 'vectorizer__max_features':[None,100,50],\n",
    "            'lda__n_components':[3,5],\n",
    "            # 'lda__learning_method':['batch','online'],\n",
    "            # 'lda__learning_decay':[0.5,0.9,0.7],\n",
    "            # 'lda__learning_offset':[2,10],\n",
    "            # 'lda__max_iter':[2,10],\n",
    "            # 'lda__batch_size':[10,50],\n",
    "            # 'lda__max_doc_update_iter':[1,5]\n",
    "            }\n",
    "pipelineb=Pipeline([('vectorizer', tfidf),('lda',lda)])\n",
    "#nmf has no scorer, can't be optimized unless specified\n",
    "# paramgridc={'nmf__n_components':['auto',None,5,10,25,50,100],\n",
    "#             'nmf__init':['random', 'nndsvd', 'nndsvda', 'nndsvdar'],\n",
    "#             # 'nmf__solver':['cd','mu'],\n",
    "#             # 'nmf__beta_loss':['frobenius', 'kullback-leibler', 'itakura-saito'],\n",
    "#             # 'nmf__tol':[0.001,0.01,0.003,0.03],\n",
    "#             # 'nmf__max_iter':[20,50,100,200],\n",
    "#             # 'nmf__alpha_W':[0.0,0.5,1],\n",
    "#             # 'nmf__alpha_H':[0.0,0.5,1],\n",
    "#             # 'nmf__l1_ratio':[0.0,0.25,0.5],\n",
    "#             # 'nmf__shuffle':[True,False],\n",
    "#             'vectorizer__stop_words':['english',list(STOPWORDS)],\n",
    "#             'vectorizer__max_features':[None,100,50,25,10]\n",
    "#             }\n",
    "# pipelinec=Pipeline([('vectorizer', cv),('nmf',nmf)])\n",
    "# paramgridd={'nmf__n_components':['auto',None,5,10,25,50,100],\n",
    "#             'nmf__init':['random', 'nndsvd', 'nndsvda', 'nndsvdar'],\n",
    "#             # 'nmf__solver':['cd','mu'],\n",
    "#             # 'nmf__beta_loss':['frobenius', 'kullback-leibler', 'itakura-saito'],\n",
    "#             # 'nmf__tol':[0.001,0.01,0.003,0.03],\n",
    "#             # 'nmf__max_iter':[20,50,100,200],\n",
    "#             # 'nmf__alpha_W':[0.0,0.5,1],\n",
    "#             # 'nmf__alpha_H':[0.0,0.5,1],\n",
    "#             # 'nmf__l1_ratio':[0.0,0.25,0.5],\n",
    "#             # 'nmf__shuffle':[True,False],\n",
    "#             'vectorizer__norm':['l1','l2', None],\n",
    "#             'vectorizer__smooth_idf':[True,False],\n",
    "#             'vectorizer__sublinear_tf':[True,False],\n",
    "#             'vectorizer__stop_words':['english',list(STOPWORDS)],\n",
    "#             'vectorizer__max_features':[None,100,50,25,10]\n",
    "#             }\n",
    "# pipelined=Pipeline([('vectorizer', tfidf),('nmf',nmf)])\n",
    "descriptionmodelspath=models_path.joinpath('description_models')\n",
    "\n",
    "xtrain,xtest=train_test_split(fulldatadf['description'],test_size=0.1,random_state=randomstate,shuffle=True)\n",
    "if retrain_cvlda:\n",
    "    gridsearcha=GridSearchCV(pipelinea,paramgrida,n_jobs=-1,cv=5,return_train_score=True)\n",
    "    gridamodel=gridsearcha.fit(xtrain)\n",
    "    joblib.dump(gridamodel,descriptionmodelspath.joinpath('gridamodel.pkl').as_posix())\n",
    "else:\n",
    "    gridamodel=joblib.load(descriptionmodelspath.joinpath('gridamodel.pkl').as_posix())\n",
    "\n",
    "if retrain_tfidflda:\n",
    "    gridsearchb=GridSearchCV(pipelineb,paramgridb,n_jobs=-1,cv=5,return_train_score=True)\n",
    "    gridbmodel=gridsearchb.fit(xtrain)\n",
    "    joblib.dump(gridbmodel,descriptionmodelspath.joinpath('gridbmodel.pkl').as_posix())\n",
    "else:\n",
    "    gridbmodel=joblib.load(descriptionmodelspath.joinpath('gridbmodel.pkl').as_posix())\n",
    "\n",
    "# gridsearchc=GridSearchCV(pipelinec,paramgridc,n_jobs=-1,cv=5,return_train_score=True)\n",
    "# gridcmodel=gridsearchc.fit(xtrain)\n",
    "# joblib.dump(gridcmodel,descriptionmodelspath.joinpath('gridcmodel.pkl').as_posix())\n",
    "\n",
    "# gridsearchd=GridSearchCV(pipelined,paramgridd,n_jobs=-1,cv=5,return_train_score=True)\n",
    "# griddmodel=gridsearchd.fit(xtrain)\n",
    "# joblib.dump(griddmodel,descriptionmodelspath.joinpath('griddmodel.pkl').as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erkenntnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage ohne Clustering der Nutzer\n",
    "### Ansätze Lea\n",
    "### Ansätze Heidi\n",
    "### Erkenntnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ansätze Lea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datawreviewsfile=datadirpath.joinpath('processed/innerJoinData.csv')\n",
    "datadf=pd.read_csv(datawreviewsfile,sep=';')\n",
    "\n",
    "datadf=datadf.drop(['biography', 'children', 'comics', 'crime', 'fantasy', 'fiction',\n",
    "       'graphic', 'historicalfiction', 'history', 'mystery', 'nonfiction',\n",
    "       'paranormal', 'poetry', 'romance', 'thriller', 'youngadult'],axis=1)\n",
    "\n",
    "\n",
    "datadf['description']=datadf.description.transform(remove_punctuation)\n",
    "datadf['genre']=datadf.genre.apply(lambda x: x.replace(\"'\",\"\")) # remove '\n",
    "datadf['genre']=datadf.genre.apply(lambda x: x.replace(\"-\",\"\")) # revome - \n",
    "datadf['genre']=datadf.genre.apply(lambda x: x.replace(\" \",\"\")) # remove spaces\n",
    "datadf['genre']=datadf.genre.apply(lambda x: x[1:-1].split(',')) # split into list\n",
    "\n",
    "\n",
    "mlbmodelpath=models_path.joinpath('mlb_model')\n",
    "mlbmodel=joblib.load(mlbmodelpath.joinpath('mlbmodel.pkl').as_posix())\n",
    "newcols=mlbmodel.classes_\n",
    "pred=mlbmodel.transform(datadf.genre)\n",
    "preddf=pd.DataFrame(pred,columns=newcols)\n",
    "datadf=datadf.join(preddf)\n",
    "\n",
    "descriptionmodelpath=models_path.joinpath('description_models')\n",
    "descmodel=joblib.load(descriptionmodelpath.joinpath('gridamodel.pkl').as_posix())\n",
    "newcols=descmodel.best_estimator_.named_steps['lda'].get_feature_names_out()\n",
    "pred=descmodel.best_estimator_.transform(datadf.description)\n",
    "preddf=pd.DataFrame(pred,columns=newcols)\n",
    "datadf=datadf.join(preddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadf.head()\n",
    "datadf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not for prediction\n",
    "# copied list:\n",
    "# ['title_x','book_id_x','book_id_mapping','Predicted Rating','book_id_y', 'title_y', 'image_url', 'url','description', 'genre','num_genres']\n",
    "# keep:'user_id_mapping' for user identification --> would be replaced by cluster representation, concern: as a numerical value implies false relationship between values\n",
    "# target: 'Actual Rating'\n",
    "dropcols=['title_x','book_id_x','book_id_mapping','Predicted Rating','book_id_y', 'title_y', 'image_url', 'url','description', 'genre','num_genres','name'] #drop author name too, as binarization too big and few duplicates\n",
    "datadf=datadf.drop(dropcols,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(datadf.drop(['Actual Rating'],axis=1),datadf['Actual Rating'],test_size=0.1,random_state=randomstate,shuffle=True)\n",
    "print(len(xtrain), len(ytrain))\n",
    "print(len(xtest),len(ytest))\n",
    "xtrain.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionmodelpath=models_path.joinpath('prediction_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modela=svm.SVR(max_iter=150)\n",
    "paramsa={'kernel':['linear','poly','rbf','sigmoid'],\n",
    "         'degree':[3,4],\n",
    "         'gamma':['auto','scale'],\n",
    "         'coef0':[0.0,0.5,1],\n",
    "         'tol':[0.001,0.003,0.01],\n",
    "         'C':[1.0,0.5,2.0],\n",
    "         'epsilon':[0.1,0.5],\n",
    "         'shrinking':[True,False]\n",
    "         }\n",
    "grida=GridSearchCV(modela,paramsa,n_jobs=-1,cv=5,return_train_score=True)\n",
    "\n",
    "modelb=DecisionTreeRegressor(random_state=randomstate)\n",
    "paramsb={'criterion':['squared_error','friedman_mse','absolute_error','poisson'],\n",
    "         'splitter':['best','random'],\n",
    "         'max_depth':[None,5,10],\n",
    "         'min_samples_split':[2,25,100],\n",
    "         'min_samples_leaf':[1,100,50],\n",
    "         'max_features':[None,'log2',5],\n",
    "         'max_leaf_nodes':[None,10,20],\n",
    "         'ccp_alpha':[0.0,0.2,0.5]\n",
    "         }\n",
    "gridb=GridSearchCV(modelb,paramsb,n_jobs=-1,cv=5,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_svr=True\n",
    "retrain_regtree=True\n",
    "if retrain_svr:\n",
    "    regsvmmodel=grida.fit(xtrain,ytrain)\n",
    "    joblib.dump(regsvmmodel,predictionmodelpath.joinpath('regsvmmodel.pkl').as_posix())\n",
    "else:\n",
    "    regsvmmodel=joblib.load(predictionmodelpath.joinpath('regsvmmodel.pkl').as_posix())\n",
    "if retrain_regtree:\n",
    "    regdtmodel=gridb.fit(xtrain,ytrain)\n",
    "    joblib.dump(regdtmodel,predictionmodelpath.joinpath('regdtmodel.pkl').as_posix())\n",
    "else:\n",
    "    regdtmodel=joblib.load(predictionmodelpath.joinpath('regdtmodel.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelc=SVC(max_iter=150,random_state=randomstate)\n",
    "paramsc={\n",
    "    'C':[1.0,0.5,2.0],\n",
    "    'kernel':['linear','poly','rbf','sigmoid'],\n",
    "    'degree':[3,4],\n",
    "    'gamma':['auto','scale'],\n",
    "    'coef0':[0.0,0.5,1],\n",
    "    'tol':[0.001,0.003,0.01],\n",
    "    'shrinking':[True,False],\n",
    "    'decision_function_shape':['ovo','ovr']\n",
    "}\n",
    "gridc=GridSearchCV(modelc,paramsc,n_jobs=-1,cv=5,return_train_score=True)\n",
    "\n",
    "modeld=DecisionTreeClassifier(random_state=randomstate)\n",
    "paramsd={\n",
    "    'criterion':['gini','entropy','log_loss'],\n",
    "    'splitter':['best','random'],\n",
    "    'max_depth':[None,5,10],\n",
    "    'min_samples_split':[2,25,100],\n",
    "    'min_samples_leaf':[1,100,50],\n",
    "    'max_features':[None,'log2',5],\n",
    "    'max_leaf_nodes':[None,10,20],\n",
    "    'ccp_alpha':[0.0,0.2,0.5]    \n",
    "}\n",
    "gridd=GridSearchCV(modeld,paramsd,n_jobs=-1,cv=5,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_svm=True\n",
    "retrain_clastree=True\n",
    "if retrain_svm:\n",
    "    classvmmodel=gridc.fit(xtrain,ytrain)\n",
    "    joblib.dump(classvmmodel,predictionmodelpath.joinpath('classsvmmodel.pkl').as_posix())\n",
    "else:\n",
    "    classvmmodel=joblib.load(predictionmodelpath.joinpath('classsvmmodel.pkl').as_posix())\n",
    "\n",
    "if retrain_clastree:\n",
    "    clasdtmodel=gridd.fit(xtrain,ytrain)\n",
    "    joblib.dump(clasdtmodel,predictionmodelpath.joinpath('clasdtmodel.pkl').as_posix())\n",
    "else:\n",
    "    clasdtmodel=joblib.load(predictionmodelpath.joinpath('clasdtmodel.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predregsvm=regsvmmodel.predict(xtest)\n",
    "predregdt=regdtmodel.predict(xtest)\n",
    "\n",
    "msesvm=mean_squared_error(ytest,predregsvm)\n",
    "msedt=mean_squared_error(ytest,predregdt)\n",
    "r2svm=r2_score(ytest,predregsvm)\n",
    "r2dt=r2_score(ytest,predregdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MSE SVM: {msesvm}, MSE DT: {msedt}')\n",
    "print(f'R2 SVM: {r2svm}, R2 DT: {r2dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predclassvm=classvmmodel.predict(xtest)\n",
    "predclasdt=clasdtmodel.predict(xtest)\n",
    "\n",
    "accsvm=accuracy_score(ytest,predclassvm)\n",
    "accdt=accuracy_score(ytest,predclasdt)\n",
    "msesvm2=mean_squared_error(ytest,predclassvm)\n",
    "msedt2=mean_squared_error(ytest,predclasdt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy SVM: {accsvm}, Accuracy DT: {accdt}')\n",
    "print(f'MSE SVM: {msesvm2}, MSE DT: {msedt2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Classification NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_studyregression=True\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(datadf.drop(['Actual Rating'],axis=1),datadf['Actual Rating'],test_size=0.1,random_state=randomstate,shuffle=True)\n",
    "xtrain,xtest,ytrain,ytest=xtrain.reset_index(drop=True),xtest.reset_index(drop=True),ytrain.reset_index(drop=True),ytest.reset_index(drop=True) #kf.split() needs reset index; if drop=False, wird index zu neuer Spalte\n",
    "studyname='prediction_regression'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "\n",
    "if retrain_studyregression:\n",
    "    def init_regmodel(trial):\n",
    "        # model definition\n",
    "        nlayers=trial.suggest_int('n_layers',1,10)\n",
    "        model=tf.keras.Sequential()\n",
    "        activation=trial.suggest_categorical('activation',['relu','tanh','sigmoid'])\n",
    "        inputshape=xtrain.shape[1]\n",
    "        model.add(tf.keras.layers.Dense(16, input_shape=(inputshape,),activation=activation))\n",
    "        for i in range(nlayers):\n",
    "            numhidden=trial.suggest_int(f'n_units_l{i}',32,128,log=True)\n",
    "            model.add(tf.keras.layers.Dense(units=numhidden,activation=activation,name=f'layer{i}'))\n",
    "        model.add(tf.keras.layers.Dense(units=1,activation='linear'))\n",
    "        model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_squared_error'])\n",
    "        return model\n",
    "\n",
    "    def reg_objective(trial:Trial,xtrain:pd.DataFrame,ytrain:pd.Series)->float:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=randomstate)\n",
    "        model=init_regmodel(trial)\n",
    "        # training and evaluation\n",
    "        msetest=[]\n",
    "        for train,test in kf.split(xtrain):\n",
    "            trdata=tf.convert_to_tensor(xtrain.loc[train])\n",
    "            trlabel=tf.convert_to_tensor(ytrain.loc[train])\n",
    "            evdata=tf.convert_to_tensor(xtrain.loc[test])\n",
    "            evlabel=tf.convert_to_tensor(ytrain.loc[test])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            model.fit(trdata,trlabel,validation_split=0.1,epochs=10, batch_size=128,verbose=0) #more verbosity seems to crash in execution\n",
    "            \n",
    "            loss,mse=model.evaluate(evdata, evlabel)\n",
    "            # predtrain=tmodel.predict(evdata)\n",
    "            # mse=tf.keras.losses.MSE(evlabel, predtrain)\n",
    "            msetest.append(mse)\n",
    "        \n",
    "        return np.mean(msetest)\n",
    "\n",
    "    storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "    if storage.exists():\n",
    "        storage.unlink()\n",
    "    else:\n",
    "        print(\"No sqlite db found\")\n",
    "\n",
    "    study=create_study(study_name=studyname,direction='minimize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False)\n",
    "    study.optimize(lambda trial: reg_objective(trial,xtrain,ytrain),n_trials=20,n_jobs=-1,show_progress_bar=True)\n",
    "\n",
    "    joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "    model = init_regmodel(study.best_trial)\n",
    "    model.fit(tf.convert_to_tensor(xtrain),tf.convert_to_tensor(ytrain),validation_split=0.1,epochs=10, batch_size=128,verbose=0)\n",
    "    model.save(thismodelpath.joinpath(f'fmodel_reg.h5').as_posix())\n",
    "    \n",
    "else:\n",
    "    model = tf.keras.models.load_model(thismodelpath.joinpath(f'fmodel_reg.h5').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrain_studyclassification=True\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(datadf.drop(['Actual Rating'],axis=1),datadf['Actual Rating'],test_size=0.1,random_state=randomstate,shuffle=True)\n",
    "xtrain,xtest,ytrain,ytest=xtrain.reset_index(drop=True),xtest.reset_index(drop=True),ytrain.reset_index(drop=True),ytest.reset_index(drop=True)\n",
    "\n",
    "studyname='prediction_classification'\n",
    "thismodelpath=models_path.joinpath(f'{studyname}')\n",
    "try: \n",
    "    thismodelpath.mkdir(exist_ok=False)\n",
    "except FileExistsError:\n",
    "    print('Directory already exists')\n",
    "if retrain_studyclassification:\n",
    "    def init_clasmodel(trial):\n",
    "        # model definition\n",
    "        nlayers=trial.suggest_int('n_layers',1,10)\n",
    "        model=tf.keras.Sequential()\n",
    "        activation=trial.suggest_categorical('activation',['relu','tanh','sigmoid'])\n",
    "        inputshape=xtrain.shape[1]\n",
    "        model.add(tf.keras.layers.Dense(16, input_shape=(inputshape,),activation=activation))\n",
    "        for i in range(nlayers):\n",
    "            numhidden=trial.suggest_int(f'n_units_l{i}',32,128,log=True)\n",
    "            model.add(tf.keras.layers.Dense(units=numhidden,activation=activation,name=f'layer{i}'))\n",
    "        model.add(tf.keras.layers.Dense(units=5,activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def clas_objective(trial:Trial,xtrain:pd.DataFrame,ytrain:pd.Series)->float:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=randomstate)\n",
    "        ytrain=datadf['Actual Rating'].apply(lambda x: x-1)\n",
    "        model=init_clasmodel(trial)\n",
    "        # training and evaluation\n",
    "        acctest=[]\n",
    "        for train,test in kf.split(xtrain):\n",
    "            trdata=tf.convert_to_tensor(xtrain.loc[train])\n",
    "            trlabel=tf.convert_to_tensor(tf.keras.utils.to_categorical(ytrain.loc[train],num_classes=5))\n",
    "            evdata=tf.convert_to_tensor(xtrain.loc[test])\n",
    "            evlabel=tf.convert_to_tensor(tf.keras.utils.to_categorical(ytrain.loc[test],num_classes=5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            model.fit(trdata,trlabel,validation_split=0.1,epochs=10, batch_size=128,verbose=0) #more verbosity seems to crash in execution\n",
    "            \n",
    "            loss,acc=model.evaluate(evdata, evlabel)\n",
    "            # predtrain=tmodel.predict(evdata)\n",
    "            # mse=tf.keras.losses.MSE(evlabel, predtrain)\n",
    "            acctest.append(acc)\n",
    "        \n",
    "        return np.mean(acctest)\n",
    "\n",
    "    storage=thismodelpath.joinpath(f\"{studyname}.db\")\n",
    "    if storage.exists():\n",
    "        storage.unlink()\n",
    "    else:\n",
    "        print(\"No sqlite db found\")\n",
    "\n",
    "    study=create_study(study_name=studyname,direction='maximize',storage=f'sqlite:///{storage.as_posix()}',load_if_exists=False)\n",
    "    study.optimize(lambda trial: clas_objective(trial,xtrain,ytrain),n_trials=20,n_jobs=-1,show_progress_bar=True)\n",
    "\n",
    "    joblib.dump(study,thismodelpath.joinpath(f'study_{study.study_name}.pkl').as_posix())\n",
    "\n",
    "    model = init_clasmodel(study.best_trial)\n",
    "    ytrain=datadf['Actual Rating'].apply(lambda x: x-1)\n",
    "    model.fit(tf.convert_to_tensor(xtrain),tf.convert_to_tensor(tf.keras.utils.to_categorical(ytrain,num_classes=5)),validation_split=0.1,epochs=10, batch_size=128,verbose=0)\n",
    "    model.save(thismodelpath.joinpath(f'fmodel_clas.h5').as_posix())\n",
    "else:\n",
    "    model=tf.keras.models.load_model(thismodelpath.joinpath(f'fmodel_clas.h5').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare models to Predicted Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ansätze Heidi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with a more detailed analysis of the merged data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspired by Leas approach to analyze the initial dataset\n",
    "\n",
    "print(f\"Number of observations: {len(merged_df)}\")\n",
    "print(f\"Number of unique book ids: {len(merged_df[\"book_id_x\"].unique())}\")\n",
    "print(f\"Number of unique user ids: {len(merged_df[\"user_id_mapping\"].unique())}\")\n",
    "\n",
    "\n",
    "#limiting the output to 2 decimal digits for average rating\n",
    "print(f\"Average actual rating: {merged_df['Actual Rating'].mean():.2f}\")\n",
    "print(f\"Average predicted rating: {merged_df['Predicted Rating'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max numnber of ratings per user and others \n",
    "# groupby user id\n",
    "merged_df_userids=merged_df[[\"user_id_mapping\",\"book_id_x\"]].groupby(by=\"user_id_mapping\").count()\n",
    "merged_df_userids=merged_df_userids.reset_index()\n",
    "merged_df_userids=merged_df_userids.rename(columns={\"book_id_x\":\"book_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"(repeat) Number of users: {len(merged_df_userids)}\")\n",
    "print(f\"Max number of books revied per user: {merged_df_userids.book_count.max()}\")\n",
    "print(f\"Min number of books revied per user: {merged_df_userids.book_count.min()}\")\n",
    "print(f\"Number of users with 1 review: {len(merged_df_userids.query('book_count == 1'))}\")\n",
    "print(f\"... in %: {round(len(merged_df_userids.query('book_count == 1'))/len(merged_df_userids),2)*100}%\") \n",
    "print(f\"Number of users with 2 review: {len(merged_df_userids.query('book_count == 2'))}\")\n",
    "print(f\"... in %: {round(len(merged_df_userids.query('book_count == 2'))/len(merged_df_userids),2)*100}%\")\n",
    "print(f\"Number of users with less than 3 reviews: {len(merged_df_userids.query('book_count < 3'))}\")\n",
    "print(f\"... in %: {round(len(merged_df_userids.query('book_count < 3'))/len(merged_df_userids),2)*100}%\")\n",
    "print(f\"Number of users with more than 1 review: {len(merged_df_userids.query('book_count >1'))}\")\n",
    "#print(f\"... in %: {round(len(merged_df_userids.query('book_count >1'))/len(merged_df_userids),2)*100}%\") #does not limit it to 1 or 2 digits after the comma\n",
    "print(f\"... in %: {len(merged_df_userids.query('book_count > 1')) / len(merged_df_userids) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by the user IDs and count the entries per userid\n",
    "id_counts = merged_df.groupby('user_id_mapping').size()\n",
    "print(id_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction, use only the data of users that have 2 or more book ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for user-ids with 2 or more entries\n",
    "\n",
    "ids_with_multiple_entries = id_counts[id_counts >= 2].index\n",
    "\n",
    "# Filter the original DataFrame to only include these IDs\n",
    "df_filtered = merged_df[merged_df['user_id_mapping'].isin(ids_with_multiple_entries)]\n",
    "print(ids_with_multiple_entries.shape)\n",
    "print(df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more analyis\n",
    "# max numnber of ratings per user and others \n",
    "# groupby user id\n",
    "\n",
    "df_filtered_userids=df_filtered[[\"user_id_mapping\",\"book_id_x\"]].groupby(by=\"user_id_mapping\").count()\n",
    "df_filtered_userids=df_filtered_userids.reset_index()\n",
    "df_filtered_userids=df_filtered_userids.rename(columns={\"book_id_x\":\"book_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifiying that now we have a dataframe where every user has reviewed at least 2 books\n",
    "print(f\"(repeat) Number of users: {len(df_filtered_userids)}\")\n",
    "print(f\"Max number of books revied per user: {df_filtered_userids.book_count.max()}\")\n",
    "print(f\"Min number of books revied per user: {df_filtered_userids.book_count.min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out a bit of information about the merged and grouped dataset\n",
    "#we have a dataframe with 5156 lines with entries of 2435 users that have at least 2 ratings, with that we can try to do some predictions\n",
    "print(f\"Number of users with more than 1 review: {len(df_filtered_userids.query('book_count >1'))}\")\n",
    "print(df_filtered_userids.columns)\n",
    "print(df_filtered_userids.shape)\n",
    "print(df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns that I believe are not needed going forward, e.g with double entry and text only, that cannot be used in the algorithm\n",
    "df_prediction = df_filtered.drop(['Predicted Rating', 'book_id_y', 'title_y', 'description', 'image_url', 'url', 'name'],axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that my code did want I wanted it to do\n",
    "print(df_prediction.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prediction.head())\n",
    "print(df_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure I need this here\n",
    "#making the columns to feature to prepare prediction\n",
    "# starting with all columns in the df are features\n",
    "X = df_prediction.copy()  # Copy the DataFrame to preserve the original\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try to visualize the data a bit \n",
    "sns.set(font_scale=1)\n",
    "sns.set_style('whitegrid')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the feature names\n",
    "feature_names = df_prediction.columns.tolist()\n",
    "print(\"Feature names:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the feature_names as an attribute to the DataFrame\n",
    "df_prediction.feature_names = df_prediction.columns.tolist()\n",
    "\n",
    "# Access the custom attribute\n",
    "print(\"Feature names attribute:\", df_prediction.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aus der Vorlesung: die ergebnisse gegen die einzelnen features plotten, und sehen ob es ein erkennbares Muster gibt\n",
    "# wie sie sie sehen, sehen sie nix, daraus kann man erst mal nix ablesen\n",
    "#next steps - wieder einen vektor pro nutzer, dann nochmal versuchen\n",
    "\n",
    "for feature in df_prediction.feature_names:\n",
    "    plt.figure(figsize=(8, 4.5))  # 8\"-by-4.5\" Figure\n",
    "    sns.scatterplot(data=df_prediction, x=feature, \n",
    "                    y='Actual Rating', hue='Actual Rating', \n",
    "                    palette='cool', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create one vector per user\n",
    "for whatever reason, what I did before does not work now anymore\n",
    "\n",
    "new try\n",
    "df_filtered is the dataframe to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered.shape)\n",
    "print(df_filtered.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try to boil it down to 1 vector per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_prediction = df_filtered.drop(['Predicted Rating', 'book_id_y', 'title_y', 'description', 'image_url', 'url', 'name'],axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_for_prediction.shape)\n",
    "print(df_for_prediction.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_metadata_df = df_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_metadata_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all columns are numeric columns\n",
    "df_for_prediction = df_for_prediction.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_for_prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating user vectors based on different methods\n",
    "- mean\n",
    "- median\n",
    "- sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create one vector per user, use different methods to create them so that we have different inputs for predictions\n",
    "#user vectors using mean\n",
    "user_vectors_mean = df_for_prediction.groupby('user_id_mapping').mean()\n",
    "print(user_vectors_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectors_sum = df_for_prediction.groupby('user_id_mapping').sum()\n",
    "print(user_vectors_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user vectors using median\n",
    "\n",
    "user_vectors_median = df_for_prediction.groupby('user_id_mapping').median()\n",
    "print(user_vectors_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the prediction\n",
    "\n",
    "Get started with user_vector_mean\n",
    "linear regression, no normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the code I wanted to run did not go through due to missing values... finding the missing values now and cleaning up\n",
    "#check the missing values per column - title_x has missing values - therefore drop that column\n",
    "missing_values = user_vectors_mean.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column title_x has missing values, so I drop it\n",
    "user_vectors_mean = user_vectors_mean.drop(columns=['title_x'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_vectors_mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, having done the other steps first, it should be possible to run the following code\n",
    "\n",
    "outcome: not good\n",
    "\n",
    "Mean Squared Error: 0.6478485122777936\n",
    "\n",
    "R-squared: 0.0412460789151855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Assuming your dataframe is called user_vector_median\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "# Assuming \"Actual Rating\" is the column you're predicting and the rest are features\n",
    "X = user_vectors_mean.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_mean['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing it all now for the next user vectore\n",
    "user_vectors_sum\n",
    "\n",
    "Result is better than with mean, but not yet satisfying\n",
    "\n",
    "Mean Squared Error: 2.8747540695971443\n",
    "\n",
    "R-squared: 0.37543279109204053\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check the missing values per column - title_x has missing values - therefore drop that column\n",
    "missing_values = user_vectors_sum.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Assuming your dataframe is called user_vector_median\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "# Assuming \"Actual Rating\" is the column you're predicting and the rest are features\n",
    "X = user_vectors_sum.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_sum['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing it all now for the next user vector\n",
    "user_vectors_median\n",
    "\n",
    "Result is better than with mean, but not yet satisfying either\n",
    "\n",
    "Mean Squared Error: 0.6683919894292838\n",
    "R-squared: 0.03712562568513289\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the missing values per column - title_x has missing values - therefore drop that column\n",
    "missing_values = user_vectors_median.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column title_x has missing values, so I drop it\n",
    "user_vectors_median = user_vectors_median.drop(columns=['title_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "\n",
    "X = user_vectors_median.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_median['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model so far was with user_vectors_sum, I will try to tweak the model based on this. \n",
    "\n",
    "1st Iteration of trying to improve: \n",
    "normalizing/ standardizing the features\n",
    "\n",
    "This is where we started:\n",
    "Mean Squared Error: 2.8747540695971443\n",
    "\n",
    "R-squared: 0.37543279109204053\n",
    "\n",
    "\n",
    "This is the result - it did not get better but rather worse\n",
    "Mean Squared Error: 2.8793309884843996\n",
    "R-squared: 0.3744384126563185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the model with scaled data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the scaled test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next try - apply ridgre regression model with regularization \n",
    "\n",
    "This is where we started:\n",
    "Mean Squared Error: 2.8747540695971443\n",
    "\n",
    "R-squared: 0.37543279109204053\n",
    "\n",
    "\n",
    "This is the result with tunining parameter alpha = 1.0 - it did get minimally better, not really worth the effort\n",
    "Mean Squared Error: 2.8737646386033306\n",
    "R-squared: 0.3756477542294959\n",
    "\n",
    "Try outs with 0.1 < alpha <10 leads to the results that R-squared does not get better than 0.375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Ridge Regression model with regularization\n",
    "ridge_model = Ridge(alpha=10)  # You can tune alpha (regularization strength)\n",
    "\n",
    "# Fit the model\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying it out with Lasso method\n",
    "\n",
    "This is where we started:\n",
    "Mean Squared Error: 2.8747540695971443\n",
    "\n",
    "R-squared: 0.37543279109204053\n",
    "\n",
    "This is the result with tunining parameter alpha = 1.0 - it did get worse\n",
    "Mean Squared Error: 3.3486685307714454\n",
    "R-squared: 0.2724704419272538\n",
    "\n",
    "Try outs with minimizing alpha lead to a result nearing 0.375 again, so also not worth the effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Lasso Regression model\n",
    "lasso_model = Lasso(alpha=0.0005)  # Tune alpha\n",
    "\n",
    "# Fit and predict\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "y_pred = lasso_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying out cross validation, to see if this improves it\n",
    "Before starting, I executed the code again for user_vectors_sum and then for the scaler, to ensure I start with the right data into the cross validation\n",
    "\n",
    "This is where we started:\n",
    "Mean Squared Error: 2.8747540695971443\n",
    "R-squared: 0.37543279109204053\n",
    "\n",
    "This is the result, with a 5fold cross validation\n",
    "Cross-validated R-squared scores: [0.4414783  0.47124763 0.43147318 0.44211813 0.30602363]\n",
    "Mean R-squared: 0.4184681729038432\n",
    "\n",
    "This is the result, with a 10fold cross validation\n",
    "Cross-validated R-squared scores: [0.31469016 0.45364744 0.47981629 0.43729431 0.4728822  0.39495923\n",
    " 0.36394261 0.49824312 0.32484085 0.41584173]\n",
    "Mean R-squared: 0.41561579448026603\n",
    "\n",
    "Increasing the folds does not improve R-squared\n",
    "\n",
    "Score with 3-fold cross validation\n",
    "Cross-validated R-squared scores: [0.46044342 0.37694054 0.422745  ]\n",
    "Mean R-squared: 0.42004298618043784 - this is so far the best result, but unfortunately not yet satisfying\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score not needed anymore after the first time\n",
    "\n",
    "\n",
    "# Perform 5-fold cross-validation with R-squared as the scoring metric\n",
    "cv_r2_scores = cross_val_score(model, X_train_scaled, y_train, cv=3, scoring='r2')\n",
    "\n",
    "# Output the individual R-squared values for each fold\n",
    "print(f'Cross-validated R-squared scores: {cv_r2_scores}')\n",
    "\n",
    "# Output the mean R-squared value across all folds\n",
    "print(f'Mean R-squared: {cv_r2_scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next approach to tweak the model - Feature selection\n",
    "let's see if all the features are actually needed - with recursive feature elimination\n",
    "start with 10 features to select\n",
    "\n",
    "not helpful at all\n",
    "\n",
    "Outcome:\n",
    "Mean Squared Error: 4.21\n",
    "R-squared: 0.09\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RFE to select the best features\n",
    "selector = RFE(model, n_features_to_select=10)\n",
    "selector = selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the datasets to only use selected features\n",
    "X_train_rfe = selector.transform(X_train_scaled)\n",
    "X_test_rfe = selector.transform(X_test_scaled)\n",
    "\n",
    "# Fit the model with selected features\n",
    "model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predict on the selected test set\n",
    "y_pred = model.predict(X_test_rfe)\n",
    "\n",
    "# evaluation for the model after feature selection\n",
    "\n",
    "rfe_mse = mean_squared_error(y_test, y_pred)\n",
    "rfe_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RFE Model - MSE: {rfe_mse:.2f}, R-squared: {rfe_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more complex models - Random Forest\n",
    "with n_estimators=100, random_state=42 the results are not helping\n",
    "\n",
    "Mean Squared Error: 3.388927720124742\n",
    "R-squared: 0.26372375650006896\n",
    "\n",
    "Adjusting the parameters does not change the numbers significantly, so also not helpful\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last try in the regression methods - polynomial features\n",
    "\n",
    "with polynomial degree = 2 the following results are received\n",
    "MSE: 3.37\n",
    "R-squared: 0.27\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Fit the model with polynomial features\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate model performance\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "# R-squared (R²)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'MSE: {mse:.2f}')\n",
    "\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbor Prediction\n",
    "Lets start with k-nearest neighbor for the prediction\n",
    "start with user_vectors_sum again, as it had the best starting values\n",
    "\n",
    "This is where we start\n",
    "Mean Squared Error: 2.8747540695971443\n",
    "R-squared: 0.37543279109204053\n",
    "\n",
    "Score with 3-fold cross validation\n",
    "Cross-validated R-squared scores: [0.46044342 0.37694054 0.422745  ]\n",
    "Mean R-squared: 0.42004298618043784 - this is so far the best result, but unfortunately not yet satisfying\n",
    "\n",
    "Results with test_size=0.2, random_state=42 and k=5\n",
    "Mean Squared Error: 3.94\n",
    "Root Mean Squared Error: 1.99\n",
    "R-squared: 0.14\n",
    "\n",
    "Changing k does not lead to better results\n",
    "\n",
    "Trying out different distances\n",
    "1st try - Minkowski distance - does not lead to any changes\n",
    "2nd try - Manhatten distance - that is increasing R-squared to 0.15 with k=15, but also not helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = user_vectors_sum.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_sum['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the KNN regressor\n",
    "k = 15\n",
    "\n",
    "# Using Minkowski distance and using different p\n",
    "#knn_minkowski = KNeighborsRegressor(n_neighbors=k, metric='minkowski', p=3)\n",
    "#knn_minkowski.fit(X_train_scaled, y_train)\n",
    "#y_pred_minkowski = knn_minkowski.predict(X_test_scaled)\n",
    "\n",
    "#knn = KNeighborsRegressor(n_neighbors=k)\n",
    "#knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "#y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Using Manhattan distance\n",
    "knn_manhattan = KNeighborsRegressor(n_neighbors=k, metric='manhattan')\n",
    "knn_manhattan.fit(X_train_scaled, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred_manhattan)\n",
    "rmse = mean_squared_error(y_test, y_pred_manhattan, squared=False)  # RMSE\n",
    "r2 = r2_score(y_test, y_pred_manhattan)\n",
    "\n",
    "# Print the results\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Multi Layer Perceptron for prediction\n",
    "start with user_vectors_sum again, as it had the best starting values\n",
    "\n",
    "This is where we start\n",
    "Mean Squared Error: 2.8747540695971443\n",
    "R-squared: 0.37543279109204053\n",
    "\n",
    "Score with 3-fold cross validation\n",
    "Cross-validated R-squared scores: [0.46044342 0.37694054 0.422745  ]\n",
    "Mean R-squared: 0.42004298618043784 - this is so far the best result, but unfortunately not yet satisfying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Separate features and target\n",
    "X = user_vectors_sum.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_sum['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the MLP regressor\n",
    "\n",
    "model = MLPRegressor(hidden_layer_sizes=(100, 50), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)  # RMSE\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'MSE: {mse:.2f}')\n",
    "print(f'RMSE: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Prediction for users with at least 3 reviews in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by the user IDs and count the entries per userid\n",
    "id_counts = merged_df.groupby('user_id_mapping').size()\n",
    "print(id_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 254 users with 3 or more ratings, all in all 794 ratings remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for user-ids with 3 or more entries\n",
    "\n",
    "ids_with_multiple_entries = id_counts[id_counts >= 3].index\n",
    "\n",
    "# Filter the original DataFrame to only include these IDs\n",
    "df_filtered3 = merged_df[merged_df['user_id_mapping'].isin(ids_with_multiple_entries)]\n",
    "print(ids_with_multiple_entries.shape)\n",
    "print(df_filtered3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##create 1 vector per user with the remaining users, re-using the code from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered3.shape)\n",
    "print(df_filtered3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to drop some columns as they do not contain text\n",
    "df_for_prediction3 = df_filtered3.drop(['Predicted Rating', 'book_id_y', 'title_y', 'title_x', 'description', 'image_url', 'url', 'name'],axis=1 )\n",
    "print(df_for_prediction3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_for_prediction3.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now boiling it down to 1 vector per user, and having 3 different approaches: mean, sum, median\n",
    "\n",
    "#user vectors using mean\n",
    "user_vectors_mean3 = df_for_prediction3.groupby('user_id_mapping').mean()\n",
    "print(user_vectors_mean3)\n",
    "print(user_vectors_mean3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user vectors using sum\n",
    "user_vectors_sum3 = df_for_prediction3.groupby('user_id_mapping').sum()\n",
    "print(user_vectors_sum3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user vectors using median\n",
    "user_vectors_median3 = df_for_prediction3.groupby('user_id_mapping').median()\n",
    "print(user_vectors_median3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with prediction - linear regression for all 3 vectors\n",
    "interestingly, the results are worse than with the other data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "\n",
    "X = user_vectors_mean3.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_mean3['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features (X) and target (y)\n",
    "\n",
    "X = user_vectors_sum3.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_sum3['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features (X) and target (y)\n",
    "\n",
    "X = user_vectors_median3.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_median3['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the Vector with method sum as it had the best result, although not good at all\n",
    "1st standardizing the data, then trying cross validation\n",
    "\n",
    "Cross validation is not helping at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardizing/ normalizing the features first, so I start with running the regression for user_vector_sum3 again\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the model with scaled data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the scaled test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import cross_val_score #not needed anymore after the first time\n",
    "\n",
    "\n",
    "# Perform 5-fold cross-validation with R-squared as the scoring metric\n",
    "cv_r2_scores = cross_val_score(model, X_train_scaled, y_train, cv=2, scoring='r2')\n",
    "\n",
    "# Output the individual R-squared values for each fold\n",
    "print(f'Cross-validated R-squared scores: {cv_r2_scores}')\n",
    "\n",
    "# Output the mean R-squared value across all folds\n",
    "print(f'Mean R-squared: {cv_r2_scores.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next try - apply Ridge Regression (run the scaler first again, just to be sure)\n",
    "start with alpha = 1\n",
    "\n",
    "also not leading to better results than R-squared ca 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import Ridge # not needed after the first time\n",
    "\n",
    "# Create a Ridge Regression model with regularization\n",
    "ridge_model = Ridge(alpha=5)  # You can tune alpha (regularization strength)\n",
    "\n",
    "# Fit the model\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared score\n",
    "\n",
    "# Output the results\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n",
    "\n",
    "# If you want to inspect the coefficients of the regression model\n",
    "coefficients = model.coef_\n",
    "print(\"Model coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying Feature selection\n",
    "\n",
    "Best result so far with R-Squared= 0.26 when n=15 features, and MSE= 3.83, but this is also not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Use RFE to select the best features\n",
    "selector = RFE(model, n_features_to_select=15)\n",
    "selector = selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the datasets to only use selected features\n",
    "X_train_rfe = selector.transform(X_train_scaled)\n",
    "X_test_rfe = selector.transform(X_test_scaled)\n",
    "\n",
    "# Fit the model with selected features\n",
    "model.fit(X_train_rfe, y_train)\n",
    "\n",
    "# Predict on the selected test set\n",
    "y_pred = model.predict(X_test_rfe)\n",
    "\n",
    "# evaluation for the model after feature selection\n",
    "\n",
    "rfe_mse = mean_squared_error(y_test, y_pred)\n",
    "rfe_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'RFE Model - MSE: {rfe_mse:.2f}, R-squared: {rfe_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying it with polynomials - not good at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "X_train_poly = poly.fit_transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "\n",
    "# Fit the model with polynomial features\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_poly)\n",
    "\n",
    "# Evaluate model performance\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "# R-squared (R²)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f'MSE: {mse:.2f}')\n",
    "\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying with K-Nearest Neighbor\n",
    "- with standard distance not helping at all\n",
    "- Manhattan distance also not helping\n",
    "- Minkowski distance also not helping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate features and target\n",
    "X = user_vectors_sum3.drop(columns=['Actual Rating'])\n",
    "y = user_vectors_sum3['Actual Rating']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the KNN regressor\n",
    "k = 45\n",
    "\n",
    "# Using Minkowski distance and using different p\n",
    "knn_minkowski = KNeighborsRegressor(n_neighbors=k, metric='minkowski', p=3)\n",
    "knn_minkowski.fit(X_train_scaled, y_train)\n",
    "y_pred_minkowski = knn_minkowski.predict(X_test_scaled)\n",
    "\n",
    "#knn = KNeighborsRegressor(n_neighbors=k)\n",
    "#knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "#y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "# Using Manhattan distance\n",
    "#knn_manhattan = KNeighborsRegressor(n_neighbors=k, metric='manhattan')\n",
    "#knn_manhattan.fit(X_train_scaled, y_train)\n",
    "#y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred_minkowski)\n",
    "rmse = mean_squared_error(y_test, y_pred_minkowski, squared=False)  # RMSE\n",
    "r2 = r2_score(y_test, y_pred_minkowski)\n",
    "\n",
    "# Print the results\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.2f}')\n",
    "print(f'R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erkenntnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allgemeine Erkenntnisse und Take-Aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufteilung"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
